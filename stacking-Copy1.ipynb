{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import datetime as dt\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.model_selection import KFold\n",
    "import catboost\n",
    "import lightgbm\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import hstack\n",
    "from numpy import vstack\n",
    "from numpy import asarray\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./custom_data/train_feature_0202.csv\")\n",
    "test = pd.read_csv(\"./custom_data/test_feature_0202.csv\")\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('problem', axis=1)\n",
    "y = train['problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X\n",
    "# y_train = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = {\"logging_level\" : \"Silent\"}\n",
    "lgb_params = {'bagging_fraction': 0.8230355010876419,\n",
    " 'feature_fraction': 0.6902913997439231,\n",
    " 'lambda_l1': 2.6372581499496346,\n",
    " 'lambda_l2': 2.514265593910581,\n",
    " 'learning_rate': 0.020157784746361176,\n",
    " 'max_depth': 12,\n",
    " 'min_child_samples': 12,\n",
    " 'min_child_weight': 38.245016899981515,\n",
    " 'min_samples_split': 1,\n",
    " 'min_split_gain': 0.054628773637090934,\n",
    " 'n_estimators': 840,\n",
    " 'num_leaves': 433,\n",
    " 'subsample': 0.5293225091635065}\n",
    "xgb_params = {'gamma': 8.712501813678685, 'max_depth': 22, 'min_child_weight': 9.863337491640031, 'n_estimators': 106, 'reg_alpha': 0.032974405578371495, 'reg_lambda': 0.0006919861676045414}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = []\n",
    "    models.append(catboost.CatBoostClassifier(**cat_params))\n",
    "    models.append(LGBMClassifier(**lgb_params))\n",
    "    models.append(xgb.XGBClassifier(**xgb_params))\n",
    "    models.append(RandomForestClassifier())\n",
    "    models.append(ExtraTreesClassifier())\n",
    "    models.append(GradientBoostingClassifier())\n",
    "    models.append(KNeighborsClassifier())\n",
    "    models.append(GaussianNB())\n",
    "    models.append(LinearDiscriminantAnalysis())\n",
    "    models.append(GaussianProcessClassifier())\n",
    "    models.append(SVC(probability = True))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect out of fold predictions form k-fold cross validation\n",
    "def get_out_of_fold_predictions(X, X_scaled, y, models):\n",
    "    meta_X, meta_y = list(), list()\n",
    "    kfold = KFold(n_splits=10, shuffle=True)\n",
    "    fscores = np.zeros((10, len(models)))\n",
    "    for total_idx, (train_ix, test_ix) in tqdm(enumerate(kfold.split(X))):\n",
    "        fold_yhats = list()\n",
    "        train_X, test_X = X[train_ix], X[test_ix]\n",
    "        train_X_scaled, test_X_scaled = X_scaled[train_ix], X_scaled[test_ix]\n",
    "        train_y, test_y = y[train_ix], y[test_ix]\n",
    "        meta_y.extend(test_y)\n",
    "        \n",
    "        scores = []\n",
    "        for model_idx, model in enumerate(models):\n",
    "            if model_idx < 6: #####\n",
    "                model.fit(train_X, train_y)\n",
    "                yhat = model.predict_proba(test_X)\n",
    "                fold_yhats.append(yhat)\n",
    "                score = roc_auc_score(test_y, yhat[:, 1], average='micro')\n",
    "                fscores[total_idx, model_idx] = score\n",
    "            else:\n",
    "                model.fit(train_X_scaled, train_y)\n",
    "                yhat = model.predict_proba(test_X_scaled)\n",
    "                fold_yhats.append(yhat)\n",
    "                score = roc_auc_score(test_y, yhat[:, 1], average='micro')\n",
    "                fscores[total_idx, model_idx] = score                \n",
    "        meta_X.append(hstack(fold_yhats))\n",
    "    return vstack(meta_X), asarray(meta_y), fscores\n",
    " \n",
    "# fit all base models on the training dataset\n",
    "def fit_base_models(X, X_scaled, y, models):\n",
    "    for model_idx, model in enumerate(models):\n",
    "        if model_idx < 4:\n",
    "            model.fit(X, y)\n",
    "        else:\n",
    "            model.fit(X_scaled, y)\n",
    " \n",
    "# fit a meta model\n",
    "def fit_meta_model(X, y):\n",
    "    model = LogisticRegression(solver='liblinear')\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    " \n",
    "# evaluate a list of models on a dataset\n",
    "def evaluate_models(X, X_scaled, y, models):\n",
    "    for model_idx, model in enumerate(models):\n",
    "        if model_idx < 4:     \n",
    "            yhat = model.predict_proba(X)[:,1]\n",
    "            auc = roc_auc_score(y, yhat)\n",
    "            print('%s: %.3f' % (model.__class__.__name__, auc*100))\n",
    "        else:\n",
    "            yhat = model.predict_proba(X_scaled)[:,1]\n",
    "            auc = roc_auc_score(y, yhat)\n",
    "            print('%s: %.3f' % (model.__class__.__name__, auc*100))\n",
    "# make predictions with stacked model\n",
    "def super_learner_predictions(X, X_scaled, models, meta_model):\n",
    "    meta_X = list()\n",
    "    for model_idx, model in enumerate(models):\n",
    "        if model_idx < 4:\n",
    "            yhat = model.predict_proba(X)\n",
    "            meta_X.append(yhat)\n",
    "        else:\n",
    "            yhat = model.predict_proba(X_scaled)\n",
    "            meta_X.append(yhat)\n",
    "    meta_X = hstack(meta_X)\n",
    "   # predict\n",
    "    return meta_model.predict_proba(meta_X)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6902913997439231, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6902913997439231\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.6372581499496346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6372581499496346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8230355010876419, subsample=0.5293225091635065 will be ignored. Current value: bagging_fraction=0.8230355010876419\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.514265593910581, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.514265593910581\n",
      "[22:37:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [32:54, 1974.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6902913997439231, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6902913997439231\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.6372581499496346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6372581499496346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8230355010876419, subsample=0.5293225091635065 will be ignored. Current value: bagging_fraction=0.8230355010876419\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.514265593910581, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.514265593910581\n",
      "[23:10:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [1:25:29, 2328.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_samples_split\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6902913997439231, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6902913997439231\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.6372581499496346, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.6372581499496346\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8230355010876419, subsample=0.5293225091635065 will be ignored. Current value: bagging_fraction=0.8230355010876419\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.514265593910581, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.514265593910581\n",
      "[00:03:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "meta_X, meta_y, fscores = get_out_of_fold_predictions(X_train.values, X_scaled_train, y_train.values, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = fit_meta_model(meta_X, meta_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "status|cat|lgb|xgb|rf\n",
    ":---|:---:|:---:|:---:|:---:\n",
    "tuning X|0.83847134|0.83117312|0.83468733|0.82470269\n",
    "tuning O|----|0.83677207|0.832428|0.81877891\n",
    "tuning O, n = 500|----|----|0.82202676|0.81936112|\n",
    "tuning X, n = 100|----|----|----|----|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 데이터 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_base_models(X_train, X_scaled_train ,y_train.values, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_models(X_test, X_scaled_test, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(fscores, columns=['cat', 'lgbm', 'xgb', 'rf', 'et', 'gbc', 'knn','nb','lda', 'gpc', 'svc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(fscores, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최대성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = super_learner_predictions(X_test, X_scaled_test, models, meta_model)\n",
    "roc_auc_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scaler = StandardScaler()\n",
    "X_scaled = new_scaler.fit_transform(X)\n",
    "test_scaled = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = super_learner_predictions(test, test_scaled, models, meta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['problem'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('0203.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['problem'].apply(lambda x: round(x*10)).value_counts().sort_index().plot.bar()\n",
    "plt.ylim(0,7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['problem'].apply(lambda x: round(x*1)).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
